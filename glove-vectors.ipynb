{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#CMU pronunciation (phoneme) dictionary\n",
    "import cmudict\n",
    "\n",
    "#Natural Language Toolkit\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "import wikipedia\n",
    "import requests\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the glove vectors from a txt file into a dictionary. This will take ~20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv with pandas\n",
    "df = pd.read_csv('glove.6B.300d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "\n",
    "#format glove vector dictionary\n",
    "glove_vecs = {key: val.values for key, val in df.T.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed things up, filter the vectors to only include words with lexical frequency > 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read SUBTLEX excel file\n",
    "subtlex = pd.read_excel('SUBTLEX.xlsx')\n",
    "\n",
    "#get wordlist column\n",
    "subtlex_wordlist=list(subtlex['Word'])\n",
    "\n",
    "#get Log10 word frequency column\n",
    "freqs=np.array(subtlex['Lg10WF'])\n",
    "\n",
    "#set word frequency dict\n",
    "word_freqs=dict(zip(subtlex_wordlist,freqs))\n",
    "\n",
    "#filter words\n",
    "keep_words=[word for word in glove_vecs if word in word_freqs and word_freqs[word]>1.5]\n",
    "\n",
    "#reset glove_vecs\n",
    "glove_vecs2=dict()\n",
    "for word in keep_words:\n",
    "    glove_vecs2[word]=glove_vecs[word]\n",
    "glove_vecs=glove_vecs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out a glove vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glove_vecs['spooky'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find semantic neighbors with correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vector for 'spooky'\n",
    "w1='spooky'\n",
    "vec1=glove_vecs[w1]\n",
    "\n",
    "#correlate vec1 with all other vectors in the set\n",
    "corrs=[]\n",
    "for word in glove_vecs:\n",
    "    corr=np.corrcoef(vec1,glove_vecs[word])[0][1]\n",
    "    corrs.append(corr)\n",
    "\n",
    "#print top 10 neighbors\n",
    "n=10\n",
    "top10=[keep_words[i] for i in np.argsort(corrs)[::-1][:n]]\n",
    "print(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize 10 word vectors as 2D coordinates with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#get vectors for 10 random words\n",
    "random.shuffle(keep_words)\n",
    "randvecs=[glove_vecs[w] for w in keep_words[:10]]\n",
    "\n",
    "\n",
    "#Dimensionality reduction with UMAP (https://umap-learn.readthedocs.io/en/latest/basic_usage.html)\n",
    "randvecs2d = umap.UMAP(n_neighbors=3,\n",
    "                      min_dist=.1,\n",
    "                      metric='euclidean').fit_transform(randvecs)\n",
    "\n",
    "#plot each word based on 2d coordinates\n",
    "for ind,coords in enumerate(randvecs2d):\n",
    "\n",
    "    word=keep_words[ind]\n",
    "    \n",
    "    x=coords[0]\n",
    "    y=coords[1]\n",
    "\n",
    "    plt.text(x,y,word,fontsize=8)\n",
    "\n",
    "#set x and y limits\n",
    "plt.xlim(np.min([v[0] for v in randvecs2d])-1,np.max([v[0] for v in randvecs2d])+1)\n",
    "plt.ylim(np.min([v[1] for v in randvecs2d])-1,np.max([v[1] for v in randvecs2d])+1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot wikipedia topics in 2D by averaging the glove vectors of words in the wiki summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=['Halloween','Thanksgiving','Zebra','Giraffe','Episodic memory','Hippocampus']\n",
    "\n",
    "topic_vectors=[]\n",
    "\n",
    "for topic in topics:\n",
    "    \n",
    "    #get wiki page\n",
    "    page = wikipedia.page(topic)\n",
    "\n",
    "    #get summary (note: change 'summary' to 'content' to scrape the whole page)\n",
    "    summary=page.summary\n",
    "    \n",
    "    #make lower case for searching vector dict\n",
    "    summary=summary.lower()\n",
    "\n",
    "    #tokenize\n",
    "    tokens=word_tokenize(summary)\n",
    "\n",
    "    #get vectors\n",
    "    topic_vecs=[glove_vecs[token] for token in tokens if token in glove_vecs]\n",
    "\n",
    "    #average vectors\n",
    "    topic_mean=np.mean(topic_vecs,axis=0)\n",
    "\n",
    "    #add to list of topic vectors\n",
    "    topic_vectors.append(topic_mean)\n",
    "    \n",
    "    \n",
    "#Dimensionality reduction with UMAP (https://umap-learn.readthedocs.io/en/latest/basic_usage.html)\n",
    "randvecs2d = umap.UMAP(n_neighbors=3,\n",
    "                      min_dist=.1,\n",
    "                      metric='euclidean').fit_transform(topic_vectors)\n",
    "\n",
    "#plot each word based on 2d coordinates\n",
    "for ind,coords in enumerate(randvecs2d):\n",
    "\n",
    "    topic=topics[ind]\n",
    "    \n",
    "    x=coords[0]\n",
    "    y=coords[1]\n",
    "\n",
    "    plt.text(x,y,topic,fontsize=8)\n",
    "\n",
    "#set x and y limits\n",
    "plt.xlim(np.min([v[0] for v in randvecs2d])-1,np.max([v[0] for v in randvecs2d])+1)\n",
    "plt.ylim(np.min([v[1] for v in randvecs2d])-1,np.max([v[1] for v in randvecs2d])+1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or do the same thing with different subreddits instead of words. First define this function again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for generate the search URL from criteria\n",
    "def get_url(search_type,criteria):   \n",
    "    url='https://api.pushshift.io/reddit/search/' + search_type + '/?'\n",
    "    for crit in criteria:\n",
    "        if crit=='score':\n",
    "            url=url + crit + criteria[crit] + '&'\n",
    "        else:\n",
    "            url=url + crit + '=' + criteria[crit] + '&'\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate lists of titles and their mean glove vectors for some different subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits=['LanguageTechnology','MachineLearning','NBA','NFL','Politics','WorldNews']\n",
    "\n",
    "#initialize search criteria\n",
    "criteria1=dict()\n",
    "\n",
    "#type (comment or submission)\n",
    "search_type='submission'\n",
    "\n",
    "#start time (m (minute), h (hour), d (day))\n",
    "criteria1['before']='0d'\n",
    "\n",
    "#end time (m (minute), h (hour), d (day))\n",
    "criteria1['after']='300d'\n",
    "\n",
    "#size of results (max=1000)\n",
    "criteria1['size']='1000'\n",
    "\n",
    "#sort\n",
    "criteria1['sort_type']='score'\n",
    "criteria1['sort']='desc'\n",
    "\n",
    "subreddit_vecs=[]\n",
    "for subreddit in subreddits:\n",
    "\n",
    "    #subreddits\n",
    "    criteria1['subreddit']=subreddit\n",
    "\n",
    "    #get urls\n",
    "    url1=get_url(search_type,criteria1)\n",
    "\n",
    "    #get submissions\n",
    "    submissions1 = requests.get(url1).json()['data']\n",
    "\n",
    "    #get vectors for submissions on subreddit1\n",
    "    vecs=[]\n",
    "    for submission in submissions1:\n",
    "\n",
    "        #get title\n",
    "        title=submission['title']\n",
    "\n",
    "        #make lower case for searching vector dict\n",
    "        title=title.lower()\n",
    "\n",
    "        #tokenize title\n",
    "        tokens=word_tokenize(title)\n",
    "\n",
    "\n",
    "        #get vectors\n",
    "        title_vecs=[glove_vecs[word] for word in tokens if word in glove_vecs]\n",
    "\n",
    "        #make sure at least one word was in the glove vector set we're using\n",
    "        if len(title_vecs)<2:\n",
    "            continue\n",
    "\n",
    "        #get mean\n",
    "        title_mean=np.mean(title_vecs,axis=0)\n",
    "\n",
    "        vecs.append(title_mean)\n",
    "        \n",
    "\n",
    "    subreddit_vecs.append(np.mean(vecs,axis=0))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce to 2D and visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality reduction with UMAP (https://umap-learn.readthedocs.io/en/latest/basic_usage.html)\n",
    "vecs2d = umap.UMAP(n_neighbors=3,\n",
    "                      min_dist=.1,\n",
    "                      metric='euclidean').fit_transform(subreddit_vecs)\n",
    "\n",
    "#plot each word based on 2d coordinates\n",
    "for ind,coords in enumerate(vecs2d):\n",
    "\n",
    "    subreddit=subreddits[ind]\n",
    "    \n",
    "    x=coords[0]\n",
    "    y=coords[1]\n",
    "\n",
    "    plt.text(x,y,subreddit,fontsize=8)\n",
    "\n",
    "#set x and y limits\n",
    "plt.xlim(np.min([v[0] for v in vecs2d])-1,np.max([v[0] for v in vecs2d])+1)\n",
    "plt.ylim(np.min([v[1] for v in vecs2d])-1,np.max([v[1] for v in vecs2d])+1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to make your own GloVe vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go Here: https://nlp.stanford.edu/projects/glove/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
